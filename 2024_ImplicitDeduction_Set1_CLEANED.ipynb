{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf936d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model cell\n",
    "'''\n",
    "# import the necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution1D\n",
    "from tensorflow.keras.layers import Dropout, Input, BatchNormalization, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import datetime\n",
    "import plotCEG\n",
    "\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06a72041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b428e93c",
   "metadata": {},
   "source": [
    "# Declare DL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Deduction_Learning(include_top=True, h_activation = 'relu', output_activation = 'linear', learn_rate = 0.001, \n",
    "                       beta2 = 0.95, loss_param = 'LogCosh'):\n",
    "    '''\n",
    "    include_top: whether to include the fully-connected layers \n",
    "    input_shape : the deep learning inputs from the input PPG signal\n",
    "    output_activation : the last regressin layer activation\n",
    "    loss_param :our argument to compile the training model\n",
    "    loss_param = 'mean_squared_error'\n",
    "    '''\n",
    "    input_shape=Input(shape=(403, 2),name= \"net_input\")\n",
    "    # Block 1\n",
    "    x = Convolution1D(filters=256, kernel_size=5, name=\"block1_conv1\")(input_shape)\n",
    "    x = BatchNormalization(axis=-1, name=\"block1_batchnorm\")(x)\n",
    "    x = Activation(h_activation, name=\"block1_act\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"block1_pool\")(x)\n",
    "    # Block 2\n",
    "    x = Convolution1D(filters=256, kernel_size=5, name=\"block2_conv1\") (x)\n",
    "    x = BatchNormalization(axis=-1, name=\"block2_batchnorm\")(x)\n",
    "    x = Activation(h_activation, name=\"block2_act\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"block2_pool\")(x)\n",
    "    # Block 3\n",
    "    x = Convolution1D(filters=512, kernel_size=5, name=\"block3_conv1\") (x)\n",
    "    x = BatchNormalization(axis=-1, name=\"block3_batchnorm\")(x)\n",
    "    x = Activation(h_activation, name=\"block3_act\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"block3_pool\")(x)\n",
    "    # Block 4\n",
    "    x = Convolution1D(filters=1024, kernel_size=5, name=\"block4_conv1\") (x)\n",
    "    x = BatchNormalization(axis=-1, name=\"block4_batchnorm\")(x)\n",
    "    x = Activation(h_activation, name=\"block4_act\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"block4_pool\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = Flatten(name=\"flatten\")(x)\n",
    "        input1 = keras.layers.Input(shape=(2,))\n",
    "        x = BatchNormalization(axis=-1, name = \"batch_norm\")(x)\n",
    "        x = keras.layers.Concatenate(axis=-1)([x, input1])\n",
    "        x = Dense(1024,activation=h_activation, name=\"FC1\")(x)\n",
    "        x = Dense(512,activation=h_activation, name=\"FC2\")(x)\n",
    "        \n",
    "        x = Dense(1,activation=output_activation, name=\"predictions\")(x)\n",
    "    # Create model.\n",
    "    model = Model(inputs=[input_shape,input1], outputs=[x])\n",
    "    \n",
    "    Optimizer=Adam(lr=learn_rate, beta_1=0.9, beta_2=beta2)\n",
    "    model.compile(loss=loss_param, optimizer=Optimizer)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_date_delta(base_pid,pred_pid):\n",
    "    base_pid_path = path_indexer_df.loc[path_indexer_df['Person No']==base_pid]['ECGidx'].values[0]\n",
    "    yymmdd = base_pid_path.split(\"/\")[-2]\n",
    "    first_day = datetime.datetime.strptime(yymmdd,'%Y%m%d')\n",
    "    \n",
    "    pred_path = path_indexer_df.loc[path_indexer_df['Person No']==pred_pid]['ECGidx'].values[0]\n",
    "    yymmdd = pred_path.split(\"/\")[-2]\n",
    "    datetime_object  = datetime.datetime.strptime(yymmdd,'%Y%m%d')\n",
    "    delta = datetime_object - first_day\n",
    "    return delta.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6eaddc",
   "metadata": {},
   "source": [
    "# File path input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a2c8753",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all = pd.read_csv(\"/data/dp2/data/justin/data/merge_all_norm_resp_20191203-20.csv\")\n",
    "merge_all = merge_all.loc[merge_all['Time']==0]\n",
    "\n",
    "path_indexer_df = pd.read_csv(\"file_path_index_2023_Norm.csv\")\n",
    "\n",
    "final_df = pd.read_pickle('/data/dp2/data/justin/Sensor_Journal_invitation/Sensor_processedData_All.pkl')  #for imaginary HbA1c\n",
    "final_df=final_df.set_index('person_time_idx')\n",
    "final_df.head()\n",
    "\n",
    "implicit_df = pd.read_pickle(\"2024_ImplicitHbA1c_set1.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eccd0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "BL_drug_cond=(merge_all['BL_drug']==0)\n",
    "BP_drug_cond=(merge_all['BP_drug']==0)\n",
    "DM_drug_cond=(merge_all['DM_drug']==0)\n",
    "DM_inject_cond=(merge_all['DM_inject']==0)\n",
    "O_drug_cond=(merge_all['O_drug']==0)\n",
    "\n",
    "noDM_drug_id = merge_all.loc[(merge_all['Time']==0)&(DM_drug_cond)&DM_inject_cond]['person_time_idx']  #不含糖尿藥物\n",
    "\n",
    "DM_drug_id = merge_all.loc[(merge_all['Time']==0)&\n",
    "                           BL_drug_cond&BP_drug_cond&\n",
    "                           (~DM_drug_cond)&DM_inject_cond&\n",
    "                           O_drug_cond]['person_time_idx']  #不含胰島素\n",
    "\n",
    "anyDM_drug_id=merge_all.loc[(merge_all['Time']==0)&(~BL_drug_cond|~BP_drug_cond|~O_drug_cond)&\n",
    "                           (~DM_drug_cond|~DM_inject_cond)]['person_time_idx'] # 有服用血糖藥 或 有胰島素注射 同時混用其他藥物\n",
    "\n",
    "\n",
    "set1_target_Base_A = merge_all.loc[(merge_all['count_time']>=2)&(merge_all['count_time']<3)&\n",
    "                     (merge_all['HbA1C']>6.5)&\n",
    "                     (merge_all['person_time_idx'].isin(noDM_drug_id))]['count'].unique()\n",
    "\n",
    "set1_target_Base_B = merge_all.loc[(merge_all['count_time']>=3)&\n",
    "                     (merge_all['person_time_idx'].isin(noDM_drug_id))]['count'].unique()\n",
    "set1_target_Base =np.concatenate([set1_target_Base_A,set1_target_Base_B])\n",
    "\n",
    "print(set1_target_Base )\n",
    "\n",
    "set2_target_Base_pids = list( merge_all.loc[(merge_all['count_time']>=3)&(merge_all['person_time_idx'].isin(DM_drug_id))]['count'].unique())\n",
    "\n",
    "\n",
    "set3_target_Base_pids = list( merge_all.loc[(merge_all['count_time']>=3)&(merge_all['person_time_idx'].isin(anyDM_drug_id))]['count'].unique())\n",
    "\n",
    "print(\"SET 1 符合條件受測者共 {} 人\".format(len(set1_target_Base)))\n",
    "print(set1_target_Base )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2bbd503",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all.loc[(merge_all['Time']==0)&\n",
    "                           BL_drug_cond&BP_drug_cond&\n",
    "                           (~DM_drug_cond)&DM_inject_cond&\n",
    "                           O_drug_cond&(merge_all['count']==0)]['Person No'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "940ff977",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set1_df=pd.read_pickle(\"/data/dp2/data/2023_NIBG_with_FFT_input/set_1_paired_PPG_waveform+feature.pkl\") \n",
    "\n",
    "\n",
    "set1_df['pair'] = set1_df['person_time_idx_base']+'_'+set1_df['person_time_idx_test']\n",
    "\n",
    "set1_target = merge_all.loc[merge_all['count'].isin(set1_target_Base)|merge_all['Person No'].isin(set1_target_Base)]['person_time_idx']\n",
    "\n",
    "set1_df = set1_df.loc[(set1_df['person_time_idx_base'].isin(set1_target))|set1_df['person_time_idx_test'].isin(set1_target)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7640b604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img_HbA1c_lst=[]\n",
    "\n",
    "for person_time_idx in set1_df['person_time_idx_base']:\n",
    "    img_HbA1c_lst.append(implicit_df.loc[implicit_df['person_time_idx_base']==person_time_idx]['implicit_HbA1c'].values[0])\n",
    "    \n",
    "set1_df['img_HbA1c']=img_HbA1c_lst\n",
    "\n",
    "set1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77d7e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "col = list(np.linspace(0,399,400).astype(int).astype(str))\n",
    "col=col+['HR','FW_50','Total_Area']\n",
    "base_col = [c+\"_base\" for c in col]\n",
    "test_col = [c+\"_test\" for c in col]\n",
    "\n",
    "for target_pid in set1_target_Base:\n",
    "    test_pid_t_idx = merge_all.loc[(merge_all['Person No']==target_pid)|(merge_all['count']==target_pid)]['person_time_idx'] #get test set pid_t_idx\n",
    "    \n",
    "    \n",
    "    X_train_s      = set1_df.loc[((set1_df['person_time_idx_base'].isin(noDM_drug_id))|(set1_df['person_time_idx_test'].isin(noDM_drug_id)))&\n",
    "                                (~set1_df['person_time_idx_test'].isin(test_pid_t_idx))&\n",
    "                                (~set1_df['person_time_idx_base'].isin(test_pid_t_idx))][test_col]\n",
    "    X_train_s_base = set1_df.loc[((set1_df['person_time_idx_base'].isin(noDM_drug_id))|(set1_df['person_time_idx_test'].isin(noDM_drug_id)))&\n",
    "                                (~set1_df['person_time_idx_test'].isin(test_pid_t_idx))&\n",
    "                                (~set1_df['person_time_idx_base'].isin(test_pid_t_idx))][base_col]\n",
    "\n",
    "\n",
    "    X_test_s      = set1_df.loc[((set1_df['person_time_idx_base'].isin(test_pid_t_idx))|(set1_df['person_time_idx_test'].isin(test_pid_t_idx)))][test_col]\n",
    "    X_test_s_base = set1_df.loc[((set1_df['person_time_idx_base'].isin(test_pid_t_idx))|(set1_df['person_time_idx_test'].isin(test_pid_t_idx)))][base_col]\n",
    "    \n",
    "    t = np.array([50,400])\n",
    "    t = t.reshape(-1, 1)\n",
    "    scalert = preprocessing.MinMaxScaler().fit(t)\n",
    "\n",
    "    Y_train = set1_df.loc[((set1_df['person_time_idx_base'].isin(noDM_drug_id))|(set1_df['person_time_idx_test'].isin(noDM_drug_id)))&\n",
    "                        (~set1_df['person_time_idx_test'].isin(test_pid_t_idx))&\n",
    "                        (~set1_df['person_time_idx_base'].isin(test_pid_t_idx))]['BG_test']\n",
    "    Y_train = np.array(Y_train).reshape(-1,1)\n",
    "    Y_train = scalert.transform(Y_train)\n",
    "\n",
    "    Y_test = set1_df.loc[((set1_df['person_time_idx_base'].isin(test_pid_t_idx))|(set1_df['person_time_idx_test'].isin(test_pid_t_idx)))]['BG_test']\n",
    "    Y_test = np.array(Y_test).reshape(-1,1)\n",
    "    Y_test = scalert.transform(Y_test)\n",
    "    \n",
    "    # reshape train data\n",
    "    X_train_r_s = np.zeros((len(X_train_s), 403, 2))\n",
    "    X_train_r_s[:, :, 0] = X_train_s\n",
    "    X_train_r_s[:, :, 1] = X_train_s_base\n",
    "\n",
    "    # reshape vali data\n",
    "    X_test_r_s = np.zeros((len(X_test_s), 403, 2))\n",
    "    X_test_r_s[:, :, 0] = X_test_s\n",
    "    X_test_r_s[:, :, 1] = X_test_s_base\n",
    "    \n",
    "    features=['BG_base','img_HbA1c']\n",
    " \n",
    "    # reshape train data\n",
    "    X_train_r_f = set1_df.loc[((set1_df['person_time_idx_base'].isin(noDM_drug_id))|(set1_df['person_time_idx_test'].isin(noDM_drug_id)))&\n",
    "                            (~set1_df['person_time_idx_test'].isin(test_pid_t_idx))&\n",
    "                            (~set1_df['person_time_idx_base'].isin(test_pid_t_idx))][features]\n",
    "\n",
    "    # reshape vali data\n",
    "    X_test_r_f = set1_df.loc[((set1_df['person_time_idx_base'].isin(test_pid_t_idx))|(set1_df['person_time_idx_test'].isin(test_pid_t_idx)))][features]\n",
    "\n",
    "    \n",
    "    ############################\n",
    "    start = time.time() # Mark start trining time\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    #fit\n",
    "    model= Deduction_Learning()\n",
    "    history = model.fit([X_train_r_s,X_train_r_f], Y_train, epochs=75, batch_size=2000,\n",
    "                        validation_data=([X_test_r_s,X_test_r_f], Y_test),verbose=0)\n",
    "\n",
    "    end = time.time() # Mark end trining time\n",
    "    # 輸出結果\n",
    "    print(\"執行時間：%f 分鐘\" % ((int(end) - int(start))/60))\n",
    "    \n",
    "    plt.figure(figsize=(6,2)) #plot training & testing loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylim(0,0.1)\n",
    "    plt.show()\n",
    "\n",
    "    #build testing result table\n",
    "    test_pred = model.predict([X_test_r_s,X_test_r_f])\n",
    "    test_pred1 = scalert.inverse_transform(test_pred)\n",
    "    test_pred1=test_pred1.reshape(-1)\n",
    "\n",
    "    train_pred  =model.predict([X_train_r_s,X_train_r_f])\n",
    "    train_pred1 = scalert.inverse_transform(train_pred)\n",
    "    train_pred1=train_pred1.reshape(-1)\n",
    "\n",
    "    Y_test = set1_df.loc[((set1_df['person_time_idx_base'].isin(test_pid_t_idx))|(set1_df['person_time_idx_test'].isin(test_pid_t_idx)))][['pair','BG_test']]\n",
    "    Y_test['pred_BG']=test_pred1\n",
    "\n",
    "    ref_BG=[]\n",
    "    pred_BG=[]\n",
    "    std=[]\n",
    "    ptile_25=[]\n",
    "    ptile_75=[]\n",
    "    delta_days=[]\n",
    "    for pair in Y_test['pair'].unique():\n",
    "        ref_BG.append(int(set1_df.loc[set1_df['pair']==pair]['BG_test'].values[0])) \n",
    "        pred_BG.append(Y_test.loc[Y_test['pair']==pair]['pred_BG'].median()) \n",
    "        std.append(Y_test.loc[Y_test['pair']==pair]['pred_BG'].std()) \n",
    "        ptile_25.append(Y_test.loc[Y_test['pair']==pair]['pred_BG'].describe()['25%']) \n",
    "        ptile_75.append(Y_test.loc[Y_test['pair']==pair]['pred_BG'].describe()['75%']) \n",
    "\n",
    "        base_pid = int(pair.split('_')[0])\n",
    "        test_pid = int(pair.split('_')[2])\n",
    "        delta_days.append(get_date_delta(base_pid,test_pid))\n",
    "\n",
    "    test_result_df = pd.DataFrame({'pair': list(Y_test['pair'].unique()), 'ref_BG':ref_BG, 'pred_BG':pred_BG , 'std':std, \"ptile_25\":ptile_25, \"ptile_75\":ptile_75,'delta_days':delta_days})  \n",
    "\n",
    "    plotCEG.plot_test(list(test_result_df['ref_BG']),list(test_result_df['pred_BG']),'Testing set {}'.format(target_pid))\n",
    "    \n",
    "    #model.save(\"temp_model_holder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a95070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df.to_pickle(\"./result_tables/2024_ImplicitDL_set1-{}\".format(target_pid)) # Save test result table\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
