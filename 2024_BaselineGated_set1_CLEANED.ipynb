{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb073fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "model cell\n",
    "'''\n",
    "# import the necessary libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution1D\n",
    "from tensorflow.keras.layers import Dropout, Input, BatchNormalization, MaxPooling1D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "#from keras.utils import multi_gpu_model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def import_tensorflow():\n",
    "    # Filter tensorflow version warnings\n",
    "    import os\n",
    "    # https://stackoverflow.com/questions/40426502/is-there-a-way-to-suppress-the-messages-tensorflow-prints/40426709\n",
    "    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "    import warnings\n",
    "    # https://stackoverflow.com/questions/15777951/how-to-suppress-pandas-future-warning\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    warnings.simplefilter(action='ignore', category=Warning)\n",
    "    import tensorflow as tf\n",
    "    tf.get_logger().setLevel('INFO')\n",
    "    tf.autograph.set_verbosity(0)\n",
    "    import logging\n",
    "    tf.get_logger().setLevel(logging.ERROR)\n",
    "    return tf\n",
    "\n",
    "import os\n",
    "\n",
    "tf = import_tensorflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f99aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b35c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cce84a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_all = pd.read_csv(\"/data/dp2/data/justin/data/merge_all_norm_resp_20191203-20.csv\")\n",
    "path_indexer_df = pd.read_csv(\"/data/dp2/data/justin/reall/file_path_index_2023_Norm.csv\")\n",
    "\n",
    "SigV500_df = pd.read_pickle('/data/dp2/data/justin/reall/2023_feature+SigV500.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e130d9",
   "metadata": {},
   "source": [
    "# Plot_CEG_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c24e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test (Y_vali1, vali_pred1,name,plot=True):\n",
    "    fig = plt.figure(figsize=(11,7),dpi=100)\n",
    "    \n",
    "    \n",
    "    ax1 = fig.add_axes([0, 0.25, 1, 0.65])\n",
    "\n",
    "    ax3 = fig.add_axes([0.15, 0.05, 0.7, 0.15])\n",
    "\n",
    "    lower_bound=70\n",
    "    upper_bound=300\n",
    "    #fig.suptitle('Horizontally stacked subplots')\n",
    "\n",
    "    #ax1.set_title(\"Test\",fontsize=15)\n",
    "    ax1.scatter(Y_vali1,vali_pred1 ,facecolors='none',edgecolors='blue',alpha=0.5)\n",
    "    ax1=plot_CEG(ax1)\n",
    "    scores ,scores_lst= cal_CEG_score(Y_vali1,vali_pred1 )\n",
    "    the_table =ax3.table(cellText=scores,colLabels=['A','B','C','D','E'],loc='center'\n",
    "                         ,cellLoc='center',fontsize=13.5,bbox=(0.15, 0, 0.7, 0.7))\n",
    "    the_table.scale(1,1.6)\n",
    "    ax3.axis(\"off\")\n",
    "\n",
    "    fig.suptitle(\"CEG Analysis of {}\".format(name),fontsize=18)\n",
    "    \n",
    "    #plt.savefig('Model_Performance_pic/test_{}_{}.png'.format(name,iteration))\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "        \n",
    "def plot_CEG(ax):\n",
    "    \n",
    "    ax.set_xlabel(\"Finger Prick Measured Concentration [mg/dl]\",fontsize=13.5)\n",
    "    ax.set_ylabel(\"Predicted Concentration [mg/dl]\",fontsize=13.5)\n",
    "    ax.set_xticks([0, 50, 100, 150, 200, 250, 300, 350, 400])\n",
    "    ax.set_yticks([0, 50, 100, 150, 200, 250, 300, 350, 400])\n",
    "    ax.set_facecolor('white')\n",
    "\n",
    "    #Set axes lengths\n",
    "    ax.set_xlim([0, 400])\n",
    "    ax.set_ylim([0, 400])\n",
    "    ax.set_aspect((400)/(400))\n",
    "\n",
    "    #Plot zone lines\n",
    "    ax.plot([0,400], [0,400], ':', c='black')                      #Theoretical 45 regression line\n",
    "    ax.plot([0, 175/3], [70, 70], '-', c='black')\n",
    "    #plt.plot([175/3, 320], [70, 400], '-', c='black')\n",
    "    ax.plot([175/3, 400/1.2], [70, 400], '-', c='black')           #Replace 320 with 400/1.2 because 100*(400 - 400/1.2)/(400/1.2) =  20% error\n",
    "    ax.plot([70, 70], [84, 400],'-', c='black')\n",
    "    ax.plot([0, 70], [180, 180], '-', c='black')\n",
    "    ax.plot([70, 290],[180, 400],'-', c='black')\n",
    "    # plt.plot([70, 70], [0, 175/3], '-', c='black')\n",
    "    ax.plot([70, 70], [0, 56], '-', c='black')                     #Replace 175.3 with 56 because 100*abs(56-70)/70) = 20% error\n",
    "    # plt.plot([70, 400],[175/3, 320],'-', c='black')\n",
    "    ax.plot([70, 400], [56, 320],'-', c='black')\n",
    "    ax.plot([180, 180], [0, 70], '-', c='black')\n",
    "    ax.plot([180, 400], [70, 70], '-', c='black')\n",
    "    ax.plot([240, 240], [70, 180],'-', c='black')\n",
    "    ax.plot([240, 400], [180, 180], '-', c='black')\n",
    "    ax.plot([130, 180], [0, 70], '-', c='black')\n",
    "\n",
    "    #Add zone titles\n",
    "    ax.text(30, 15, \"A\", fontsize=15)\n",
    "    #ax.text(370, 260, \"B\", fontsize=15)\n",
    "    #ax.text(280, 370, \"B\", fontsize=15)\n",
    "    #ax.text(160, 370, \"C\", fontsize=15)\n",
    "    ax.text(160, 15, \"C\", fontsize=15)\n",
    "    ax.text(30, 140, \"D\", fontsize=15)\n",
    "    #ax.text(370, 120, \"D\", fontsize=15)\n",
    "    #ax.text(30, 370, \"E\", fontsize=15)\n",
    "    #ax.text(370, 15, \"E\", fontsize=15)\n",
    "    \n",
    "    return ax\n",
    "\n",
    "def cal_CEG_score (ref_values1,pred_values1):\n",
    "    zone1 = [0] * 5\n",
    "    for i in range(len(ref_values1)):\n",
    "        if (ref_values1[i] <= 70 and pred_values1[i] <= 70) or (pred_values1[i] <= 1.2*ref_values1[i] and pred_values1[i] >= 0.8*ref_values1[i]):\n",
    "            zone1[0] += 1    #Zone A\n",
    " \n",
    "        elif (ref_values1[i] >= 180 and pred_values1[i] <= 70) or (ref_values1[i] <= 70 and pred_values1[i] >= 180):\n",
    "            zone1[4] += 1    #Zone E\n",
    " \n",
    "        elif ((ref_values1[i] >= 70 and ref_values1[i] <= 290) and pred_values1[i] >= ref_values1[i] + 110) or ((ref_values1[i] >= 130 and ref_values1[i] <= 180) and (pred_values1[i] <= (7/5)*ref_values1[i] - 182)):\n",
    "            zone1[2] += 1    #Zone C\n",
    "        elif (ref_values1[i] >= 240 and (pred_values1[i] >= 70 and pred_values1[i] <= 180)) or (ref_values1[i] <= 175/3 and pred_values1[i] <= 180 and pred_values1[i] >= 70) or ((ref_values1[i] >= 175/3 and ref_values1[i] <= 70) and pred_values1[i] >= (6/5)*ref_values1[i]):\n",
    "            zone1[3] += 1    #Zone D\n",
    "        else:\n",
    "            zone1[1] += 1    #Zone B\n",
    "    \n",
    "    z1=zone1\n",
    "    z1_all = z1[0]+z1[1]+z1[2]+z1[3]+z1[4]\n",
    "    z1_a = round((z1[0]/z1_all)*100,1)\n",
    "    z1_b = round((z1[1]/z1_all)*100,1)\n",
    "    z1_c = round((z1[2]/z1_all)*100,1)\n",
    "    z1_d = round((z1[3]/z1_all)*100,1)\n",
    "    z1_e = round((z1[4]/z1_all)*100,1)\n",
    "    data=[[str(z1_a)+'%',str(z1_b)+'%',str(z1_c)+'%',str(z1_d)+'%',str(z1_e)+'%']]    \n",
    "    \n",
    "    return data,[z1_a , z1_b , z1_c , z1_d , z1_e]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c761cc0",
   "metadata": {},
   "source": [
    "# Declare-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42661de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Deduction_Learning(include_top=True, h_activation = 'relu', output_activation = 'linear', learn_rate = 0.00001, \n",
    "                       beta2 = 0.95, loss_param = 'mean_squared_error'):\n",
    "    '''\n",
    "    include_top: whether to include the 3 fully-connected layers at the top of the network.\n",
    "    input_shape : the deep learning inputs from the input PPG signal\n",
    "    output_activation : the last regressin layer activation\n",
    "    loss_param :our argument to compile the training model\n",
    "    loss_param = 'mean_squared_error'\n",
    "    '''\n",
    "    input_shape=Input(shape=(413, 1),name= \"net_input\")\n",
    "    # Block 1\n",
    "    x = Convolution1D(filters=256/2, kernel_size=10, name=\"block1_conv1\")(input_shape)\n",
    "    x = BatchNormalization(axis=-1, name=\"block1_batchnorm\")(x)\n",
    "    x = Activation(h_activation, name=\"block1_act\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"block1_pool\")(x)\n",
    "    # Block 2\n",
    "    x = Convolution1D(filters=256/2, kernel_size=10, name=\"block2_conv1\") (x)\n",
    "    x = BatchNormalization(axis=-1, name=\"block2_batchnorm\")(x)\n",
    "    x = Activation(h_activation, name=\"block2_act\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"block2_pool\")(x)\n",
    "    # Block 3\n",
    "    x = Convolution1D(filters=512/2, kernel_size=10, name=\"block3_conv1\") (x)\n",
    "    x = BatchNormalization(axis=-1, name=\"block3_batchnorm\")(x)\n",
    "    x = Activation(h_activation, name=\"block3_act\")(x)\n",
    "    x = MaxPooling1D(pool_size=2, name=\"block3_pool\")(x)\n",
    "\n",
    "    # regression block\n",
    "    if include_top:\n",
    "        x = Flatten(name=\"flatten\")(x)\n",
    "        #input1 = keras.layers.Input(shape=(1,))\n",
    "        x = BatchNormalization(axis=-1, name = \"batch_norm\")(x)\n",
    "        #x = keras.layers.Concatenate(axis=-1)([x, input1])\n",
    "        x = Dense(2048/2,activation=h_activation, name=\"FC1\")(x)\n",
    "        x = Dense(1024/2,activation=h_activation, name=\"FC2\")(x)\n",
    "        x = Dense(1,activation=output_activation, name=\"predictions\")(x)\n",
    "    # Create model.\n",
    "    model = Model(inputs=[input_shape], outputs=[x])\n",
    "    \n",
    "    Optimizer=Adam(lr=learn_rate, beta_1=0.9, beta_2=beta2)\n",
    "    model.compile(loss=loss_param, optimizer=Optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440488ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data_df=pd.read_pickle('2023_DL_SinglePretest_Set1_noDM_Drug.pkl')\n",
    "data_df =SigV500_df\n",
    "\n",
    "BL_drug_cond=(merge_all['BL_drug']==0)\n",
    "BP_drug_cond=(merge_all['BP_drug']==0)\n",
    "DM_drug_cond=(merge_all['DM_drug']==0)\n",
    "DM_inject_cond=(merge_all['DM_inject']==0)\n",
    "O_drug_cond=(merge_all['O_drug']==0)\n",
    "\n",
    "noDM_drug_id = merge_all.loc[(merge_all['Time']==0)&(DM_drug_cond)&DM_inject_cond]['person_time_idx']  #不含糖尿藥物\n",
    "\n",
    "DM_drug_id = merge_all.loc[(merge_all['Time']==0)&\n",
    "                           BL_drug_cond&BP_drug_cond&\n",
    "                           (~DM_drug_cond)&DM_inject_cond&\n",
    "                           O_drug_cond]['person_time_idx']  #不含胰島素\n",
    "\n",
    "anyDM_drug_id=merge_all.loc[(merge_all['Time']==0)&(~BL_drug_cond|~BP_drug_cond|~O_drug_cond)&\n",
    "                           (~DM_drug_cond|~DM_inject_cond)]['person_time_idx'] # 有服用血糖藥 或 有胰島素注射 同時混用其他藥物\n",
    "\n",
    "\n",
    "\n",
    "set1_target_Base_pids = list( merge_all.loc[(merge_all['count_time']>=2)&(merge_all['HbA1C']>6.5)&(merge_all['person_time_idx'].isin(noDM_drug_id))]['count'].unique())\n",
    "print(set1_target_Base_pids )\n",
    "\n",
    "set2_target_Base_pids = list( merge_all.loc[(merge_all['count_time']>=3)&(merge_all['person_time_idx'].isin(DM_drug_id))]['count'].unique())\n",
    "\n",
    "\n",
    "set3_target_Base_pids = list( merge_all.loc[(merge_all['count_time']>=3)&(merge_all['person_time_idx'].isin(anyDM_drug_id))]['count'].unique())\n",
    "\n",
    "print(\"SET 1 符合條件受測者共 {} 人\".format(len(set1_target_Base_pids)))\n",
    "print(set1_target_Base_pids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f904d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1=['HR','Age','FW_50','Total_Area',\n",
    "     'LFP_valley','HFP_valley','pNN20_valley',\n",
    "    'SDNN_valley','TP_valley','UT','Area50',\n",
    "    'Total_Area/HR','area']\n",
    "data_df[f1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a044dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set1_target_Base_pids=[897.0, 1009.0, 245.0, 342.0, 1304.0]\n",
    "\n",
    "train_set=[ 235.0, 897.0, 1009.0, 245.0, 342.0, 1768.0, 1628.0, 920.0, 1304.0, 789.0, 1486.0, 1725.0, 1726.0, 1592.0, 1891.0, 745.0]\n",
    "\n",
    "\n",
    "f1=['HR','Age','FW_50','W_cir',\n",
    "     'LFP_valley','HFP_valley','pNN20_valley',\n",
    "    'SDNN_valley','TP_valley','UT','Area50',\n",
    "    'Total_Area/HR','area']\n",
    "\n",
    "#f1_lst=[f+\"_lst\" for f in f1 ]\n",
    "\n",
    "f2=['last_BG']\n",
    "\n",
    "\n",
    "feature_scaler = preprocessing.MinMaxScaler().fit(data_df[f1])\n",
    "\n",
    "for target_pid in set1_target_Base_pids:\n",
    "    \n",
    "    print('Test Target {}'.format(target_pid))\n",
    "    test_pid_t_idx = merge_all.loc[(merge_all['Person No'].isin([target_pid]))|(merge_all['count'].isin([target_pid]))]['person_time_idx'] #get test set pid_t_idx\n",
    "    group = merge_all.loc[((merge_all['Person No'].isin(train_set))|(merge_all['count'].isin(train_set)))&(merge_all['HbA1C']<7.5)]['person_time_idx'] ## Use >=7.5 for Gated(H) <7.5 for Gated(L)\n",
    "    \n",
    "    train_pid_t_idx = data_df.loc[(data_df['person_time_idx'].isin(group))&(~data_df['person_time_idx'].isin(test_pid_t_idx))]['person_time_idx']\n",
    "    \n",
    "    X_train_s = data_df.loc[data_df['person_time_idx'].isin(train_pid_t_idx)].iloc[:,21:421]\n",
    "    X_test_s  = data_df.loc[data_df['person_time_idx'].isin(test_pid_t_idx)].iloc[:,21:421]\n",
    "\n",
    "    sample_weight = data_df.loc[(data_df['person_time_idx'].isin(group))&(~data_df['person_time_idx'].isin(test_pid_t_idx))]['BG'].astype(int)\n",
    "    _sample_weight=[]\n",
    "    for w in sample_weight:\n",
    "        if (w <=120) & (w >=80) :\n",
    "            _sample_weight.append( 1)\n",
    "        else:\n",
    "             _sample_weight.append( 1 )\n",
    "\n",
    "    _sample_weight=pd.Series(_sample_weight)\n",
    "\n",
    "    \n",
    "\n",
    "    t = data_df['BG']\n",
    "    t = t.values.reshape(-1, 1)\n",
    "    scaler_BG = preprocessing.MinMaxScaler().fit(t)\n",
    "\n",
    "    t1 = data_df[f1] \n",
    "\n",
    "    scaler_feat = preprocessing.MinMaxScaler().fit(t1)\n",
    "\n",
    "\n",
    "    Y_train = data_df.loc[data_df['person_time_idx'].isin(train_pid_t_idx)][['BG']]\n",
    "    Y_train = scaler_BG.transform(Y_train)\n",
    "\n",
    "    Y_test = data_df.loc[data_df['person_time_idx'].isin(test_pid_t_idx)][['BG']]\n",
    "    Y_test = scaler_BG.transform(Y_test)\n",
    "\n",
    "    # reshape train data\n",
    "    X_train_r_feat = data_df.loc[data_df['person_time_idx'].isin(train_pid_t_idx)][f1]\n",
    "    X_train_r_feat = scaler_feat.transform(X_train_r_feat)\n",
    "\n",
    "\n",
    "    # reshape vali data\n",
    "    X_test_r_feat = data_df.loc[data_df['person_time_idx'].isin(test_pid_t_idx)][f1]\n",
    "    X_test_r_feat = scaler_feat.transform(X_test_r_feat)\n",
    "\n",
    "\n",
    "    #combine normalized features with signal\n",
    "    X_train_s=X_train_s.reset_index(drop=True)\n",
    "    X_train_s=X_train_s.merge(pd.DataFrame(X_train_r_feat,columns=f1),left_index=True, right_index=True)\n",
    "\n",
    "    X_test_s=X_test_s.reset_index(drop=True)\n",
    "    X_test_s=X_test_s.merge(pd.DataFrame(X_test_r_feat,columns=f1),left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "    # reshape train data\n",
    "    X_train_r_s = np.zeros((len(X_train_s), 413, 1))\n",
    "    X_train_r_s[:, :, 0] = X_train_s\n",
    "\n",
    "\n",
    "\n",
    "    # reshape vali data\n",
    "    X_test_r_s = np.zeros((len(X_test_s), 413, 1))\n",
    "    X_test_r_s[:, :, 0] = X_test_s\n",
    "\n",
    "\n",
    "    start = time.time() # Mark start trining time\n",
    "\n",
    "    keras.backend.clear_session()\n",
    "\n",
    "    #fit\n",
    "    callback = EarlyStopping(monitor='val_loss', patience=20,restore_best_weights=True)\n",
    "\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    with strategy.scope():\n",
    "        model= Deduction_Learning()\n",
    "    \n",
    "    print(\"Train: \",X_train_r_s.shape,Y_train.shape,\"Test: \",X_test_s.shape,Y_test.shape)\n",
    "    history = model.fit([X_train_r_s], Y_train, epochs=250,\n",
    "                        batch_size=500,validation_data=([X_test_r_s], Y_test),\n",
    "                        verbose=0,sample_weight=_sample_weight) #, callbacks=[callback]\n",
    "\n",
    "    end = time.time() # Mark end trining time\n",
    "    # 輸出結果\n",
    "    print(\"執行時間：%f 分鐘\" % ((int(end) - int(start))/60))\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(6,2)) #plot training & testing loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylim(0,0.15)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    train_pred  = model.predict([X_train_r_s])\n",
    "    train_pred1 = scaler_BG.inverse_transform(train_pred)\n",
    "    train_pred1 =  train_pred1.reshape(-1)\n",
    "\n",
    "    Y_train = data_df.loc[data_df['person_time_idx'].isin(train_pid_t_idx)][['person_time_idx','BG']]\n",
    "    Y_train['pred_BG'] = train_pred1\n",
    "\n",
    "    train_result_df = pd.DataFrame()\n",
    "    ref_BG=[]\n",
    "    pred_BG=[]\n",
    "    pid_t_idx =[]\n",
    "    for pid in Y_train['person_time_idx'].unique():\n",
    "        pid_t_idx.append(pid)\n",
    "        ref_BG.append(Y_train.loc[Y_train['person_time_idx']==pid]['BG'].median()) \n",
    "        pred_BG.append(Y_train.loc[Y_train['person_time_idx']==pid]['pred_BG'].median()) \n",
    "\n",
    "    train_result_df['person_time_idx']=pid_t_idx\n",
    "    train_result_df['pred_BG']=pred_BG\n",
    "    train_result_df['ref_BG']=ref_BG\n",
    "\n",
    "    plot_test(list(Y_train['BG']),list(Y_train['pred_BG']),'Training result')\n",
    "\n",
    "\n",
    "    #build testing result table\n",
    "    test_pred = model.predict([X_test_r_s])\n",
    "    test_pred1 = scaler_BG.inverse_transform(test_pred)\n",
    "    test_pred1=test_pred1.reshape(-1)\n",
    "    ref_BG\n",
    "\n",
    "    Y_test = data_df.loc[data_df['person_time_idx'].isin(test_pid_t_idx)][['person_time_idx','BG']]\n",
    "    Y_test['pred_BG']=test_pred1\n",
    "\n",
    "    ref_BG=[]\n",
    "    pred_BG=[]\n",
    "    std=[]\n",
    "    ptile_25=[]\n",
    "    ptile_75=[]\n",
    "    passed=[]\n",
    "    for pid_t_idx in test_pid_t_idx:\n",
    "        try:\n",
    "            ref_BG.append(int(data_df.loc[data_df['person_time_idx']==pid_t_idx]['BG'].values[0])) \n",
    "            pred_BG.append(Y_test.loc[Y_test['person_time_idx']==pid_t_idx]['pred_BG'].median()) \n",
    "            std.append(Y_test.loc[Y_test['person_time_idx']==pid_t_idx]['pred_BG'].std()) \n",
    "            ptile_25.append(Y_test.loc[Y_test['person_time_idx']==pid_t_idx]['pred_BG'].describe()['25%']) \n",
    "            ptile_75.append(Y_test.loc[Y_test['person_time_idx']==pid_t_idx]['pred_BG'].describe()['75%']) \n",
    "        except:\n",
    "            passed.append(pid_t_idx)\n",
    "\n",
    "    _test_pid_t_idx=test_pid_t_idx.loc[~test_pid_t_idx.isin(passed)]\n",
    "    test_result_df = pd.DataFrame({'test_pid_t_idx':_test_pid_t_idx, 'ref_BG':ref_BG, 'pred_BG':pred_BG , 'std':std, \"ptile_25\":ptile_25, \"ptile_75\":ptile_75})  \n",
    "\n",
    "    test_result_df.to_pickle(\"./result_tables/2024 NoPretest_Set1_downsized_gatedL- {}\".format(target_pid)) # Save test result table\n",
    "\n",
    "    plot_test(list(test_result_df['ref_BG']),list(test_result_df['pred_BG']),'Testing set {}'.format(target_pid))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddd8801",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df=pd.DataFrame()\n",
    "for pid in set1_target_Base_pids:\n",
    "    temp_df = pd.read_pickle(\"./result_tables/2024 NoPretest_Set1_downsized- {}\".format(pid))\n",
    "    result_df = pd.concat([result_df,temp_df])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436ab761",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test(list(result_df['ref_BG']),list(result_df['pred_BG']),'Training result')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d35b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "plt.figure(figsize=(6,2)) #plot training & testing loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.ylim(0,0.1)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bc8fb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdf4fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c31756",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
